{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419011ab-dc9f-4746-a7fc-f1a34d4f3f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ef18b-0d54-455d-9696-1f651f88b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b8cb9-af89-4de0-895d-7569647ced53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdebfc3-f398-4b61-b77d-9c309f2aa6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_EXAMPLES_ENTS = \"\"\"\n",
    "                  EXAMPLE#1\n",
    "                   QUESTION: What does DiCaprio's full name sound like?\n",
    "                   CANDIDATE ENTITIES: \n",
    "                   Q116673393: Sibon irmelindicaprioae -- species of snake\n",
    "                    Q38111: Leonardo DiCaprio -- American actor and film producer (born 1974)\n",
    "                    Q11461: sound -- vibration that propagates as an acoustic wave\n",
    "                    Q36860035: DiCaprio -- family name\n",
    "                    Q25349951: Martin Scorsese and Leonardo DiCaprio -- collaborations\n",
    "                    Q56653813: DiCaprio 2 -- album by J.I.D\n",
    "                   RELEVANT ENTITIES: Q38111.\n",
    "                   \n",
    "                   \n",
    "                   EXAMPLE#2\n",
    "                   QUESTION: In which city near Moscow is the new Jerusalem monastery located?\n",
    "                    CANDIDATE ENTITIES: \n",
    "                    Q55502: Kingdom of Jerusalem -- medieval Christian kingdom in the Middle East\n",
    "                    Q773979: New Jerusalem Monastery -- monastery in Moscow Oblast, Russia\n",
    "                    Q1218: Jerusalem -- city in the Middle East, holy to the three Abrahamic religions\n",
    "                    Q649: Moscow -- capital and most populous city of Russia\n",
    "                    Q515: city -- large human settlement\n",
    "                    Q13164: Moscow State University -- university in Moscow, Russia\n",
    "                    Q6760: UTC+03:00 -- identifier for a time offset from UTC of +3\n",
    "                    Q10540001: Jerusalem -- family name                    \n",
    "                    RELEVANT ENTITIES: Q773979, Q649.\n",
    "\n",
    "                    \n",
    "                    EXAMPLE#3\n",
    "                    QUESTION: What capital stands on the banks of Potalaka?\n",
    "                    CANDIDATE ENTITIES: \n",
    "                    Q11626848: Mount Potalaka -- the mythical dwelling of the Buddhist bodhisattva Avalokiteśvara, said to exist in India\n",
    "                    Q22687: bank -- financial institution that accepts deposits\n",
    "                    Q60756888: Potalaka Guanyin -- sculpture by unknown artist (1965.556)\n",
    "                    Q179444: Potomac River -- river in the mid-Atlantic United States\n",
    "                    Q193893: capital -- upper part of a column (architecture)\n",
    "                    Q60: New York City -- most populous city in the United States\n",
    "                    Q5119: capital city -- primary governing city of a top-level (country) or first-level and second-level subdivision (country, state, province, regency, etc) political entity\n",
    "                    **Reasoning:**  \n",
    "                    - \"Capital\" refers to a **governing city**, not architectural elements (Q193893).  \n",
    "                    - \"Potalaka\" (Q11626848) is a **mythical** location, so no real-world capital is directly linked.  \n",
    "                    - Since no entity directly matches the question, **the most general applicable concept** is \"capital city\" (Q5119).\n",
    "                    RELEVANT ENTITIES: Q179444, Q5119.\n",
    "                    \n",
    "                    \n",
    "                    EXAMPLE#4 \n",
    "                    QUESTION: What is the official language of Brazil?  \n",
    "                    CANDIDATE ENTITIES:  \n",
    "                    Q750553: Spanish language -- Romance language originating in Spain  \n",
    "                    Q5146: Portuguese language -- Romance language, official in Portugal and Brazil  \n",
    "                    Q155: Brazil -- country in South America  \n",
    "                    Q483110: Brasília -- capital city of Brazil\n",
    "                    **Reasoning**:  \n",
    "                    - \"Official language\" refers to the main language used for governance.  \n",
    "                    - The description of Q5146 explicitly states it is official in Brazil.  \n",
    "                    - Q750553 (Spanish) is incorrect because it is **not** the official language.  \n",
    "                    - Q155 and Q483110 are related but not **languages**. \n",
    "                    RELEVANT ENTITIES: Q5146.  \n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_ENTS = f\"\"\"You are an experienced WikiData user. \n",
    "                   You receive a QUESTION and a SET of extracted Wikidata CANDIDATE ENTITIES, retrieved from an index based on similarity between the QUESTION and the ENTITY. \n",
    "                   Each entity has a wikidata ID, label and description in a form of Wikidata ID: label -- description. \n",
    "                   Your task is to select only the most semantically relevant entities based on their labels and descriptions.\n",
    "                   \n",
    "                   Instruction:\n",
    "                    -- Ignore entities that are loosely related, homonyms, or have indirect contextual links to the QUESTION.\n",
    "                    -- If no entity perfectly matches the question, select the best available entity based on contextual closeness.\n",
    "                    -- If multiple entities seem relevant, prefer those with a stronger direct connection to the query subject.\n",
    "                    -- If there are ambiguous entities that relate to the different countries, ALWAYS choose the entity that is related to Russia, based on description.\n",
    "                    -- Some questions may have multiple correct entities. Return all entities that are directly relevant. Do NOT include entities that are only indirectly related (e.g., broad categories like 'city' instead of a specific location).\n",
    "                    -- DO NOT reject entity, if it fits to the entity mentioned in query, but DOES NOT DIRECTLY answer the question.\n",
    "                    -- Always include reasoning in the beginning of the responce.\n",
    "                    -- Always place \"RELEVANT ENTITIES: ...\" at the end of your response.\n",
    "\n",
    "                   Here are examples:\n",
    "                   {FEW_SHOT_EXAMPLES_ENTS}\n",
    "                   \"\"\"\n",
    "\n",
    "USER_PROMPT_ENTS = \"\"\"QUESTION: {question}\\nCANDIDATE ENTITIES:\\n{candidate_list_str}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c29b6e-84b7-451a-ad2b-95c5c1acced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_EXAMPLES_PROP = \"\"\"\n",
    "                    EXAMPLE#1\n",
    "                    QUESTION: How much does Nurmagomedov weigh?\n",
    "                    CANDIDATE PROPERTIES:\n",
    "                    P166: award received -- award or recognition received by a person, organization or creative work\n",
    "                    P54: member of sports team -- sports teams or clubs that the subject represents or represented\n",
    "                    P1082: population -- number of people inhabiting the place; number of people of subject\n",
    "                    P1351: number of points/goals/set scored -- goals / points scored in a match or an event used as qualifier to the participant. Use P1358 for league points.\n",
    "                    P1350: number of matches played/races/starts -- matches or games a player or a team played during an event. Also a total number of matches a player officially appeared in during the whole career.\n",
    "                    P2121: prize money -- amount in a specific currency\n",
    "                    P585: point in time -- date something took place, existed or a statement was true; for providing time use the \"refine date\" property (P4241)\n",
    "                    P2067: mass -- mass (in colloquial usage also known as weight) of the item\n",
    "                    P1412: languages spoken, written or signed -- language(s) that a person or a people speaks, writes or signs, including the native language(s)\n",
    "                    P2046: area -- area occupied by an object\n",
    "                    RELEVANT PROPERTIES: P2067\n",
    "\n",
    "\n",
    "                    EXAMPLE#2\n",
    "                    QUESTION: Where is the Academy of Sciences of Armenia located\n",
    "                    CANDIDATE PROPERTIES:\n",
    "                    P463: member of -- organization, club or musical group to which the subject belongs. Do not use for membership in ethnic or social groups, nor for holding a political position, such as a member of parliament (use P39 for that)\n",
    "                    P31: instance of -- that class of which this subject is a particular example and member; different from P279 (subclass of); for example: K2 is an instance of mountain; volcano is a subclass of mountain (and an instance of volcanic landform)\n",
    "                    P19: place of birth -- most specific known birth location of a person, animal or fictional character\n",
    "                    P580: start time -- time an entity begins to exist or a statement starts being valid\n",
    "                    P582: end time -- moment when an entity ceases to exist or a statement stops being valid\n",
    "                    P69: educated at -- educational institution attended by subject\n",
    "                    P159: headquarters location -- city or town where an organization's headquarters is or has been situated. Use P276 qualifier for specific building\n",
    "                    P585: point in time -- date something took place, existed or a statement was true; for providing time use the \"refine date\" property (P4241)\n",
    "                    RELEVANT PROPERTIES: P159\n",
    "\n",
    "\n",
    "                    EXAMPLE#3\n",
    "                    QUESTION: What is the singing voice of Dmitri Hvorostovsky?\n",
    "                    CANDIDATE PROPERTIES:\n",
    "                    P412: voice type -- person's voice type. expected values: soprano, mezzo-soprano, contralto, countertenor, tenor, baritone, bass (and derivatives)\n",
    "                    P725: voice actor -- performer of a spoken role in a creative work such as animation, video game, radio drama, or dubbing over [use \"character role\" (P453) as qualifier] [use \"cast member\" (P161) for live acting]\n",
    "                    P175: performer -- actor, musician, band or other performer associated with this role or musical work\n",
    "                    P453: character role -- specific role played or filled by subject -- use only as qualifier of \"cast member\" (P161), \"voice actor\" (P725)\n",
    "                    P179: part of the series -- series which contains the subject\n",
    "                    P674: characters -- characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games)\n",
    "                    P1441: present in work -- this (fictional or fictionalized) entity, place, or person appears in that work as part of the narration (use P2860 for works citing other works, P361/P1433 for works being part of other works, P1343 for entities described in non-fictional accounts)\n",
    "                    **Reasoning:** \n",
    "                    - the question asks about voice's characteristics of some person\n",
    "                    - P412 is relevant, because it's a property described a voice type, which is voice's characteristic\n",
    "                    - P725, P175, P674 represent properties described a person role in something, not related to voice\n",
    "                    - other properties also represent something not similar to voice \n",
    "                    RELEVANT PROPERTIES: P412\n",
    "                   \n",
    "                   \n",
    "                    EXAMPLE#4\n",
    "                    QUESTION: Who was S. V. Mikhalkov's co-author in writing the text of the anthem of the Soviet Union?\n",
    "                    CANDIDATE PROPERTIES:\n",
    "                    P31: instance of -- that class of which this subject is a particular example and member; different from P279 (subclass of); for example: K2 is an instance of mountain; volcano is a subclass of mountain (and an instance of volcanic landform)\n",
    "                    P86: composer -- person(s) who wrote the music [for lyricist, use \"lyrics by\" (P676)]\n",
    "                    P1412: languages spoken, written or signed -- language(s) that a person or a people speaks, writes or signs, including the native language(s)\n",
    "                    P92: main regulatory text -- text setting the main rules by which the subject is regulated\n",
    "                    P50: author -- main creator(s) of a written work (use on works, not humans); use P2093 (author name string) when Wikidata item is unknown or does not exist\n",
    "                    P155: follows -- immediately prior item in a series of which the subject is a part, preferably use as qualifier of P179 [if the subject has replaced the preceding item, e.g. political offices, use \"replaces\" (P1365)]\n",
    "                    P676: lyricist -- author of song lyrics\n",
    "                    **Reasoning:** \n",
    "                    - the question asks about an co-author of a some national anthem's text, not the music itself!\n",
    "                    - description of P676 makes it clear that it represent \"author of song lyrics\" - that's what we needed\n",
    "                    - P31's description is too general and not correspond to songs's author\n",
    "                    - P86, P50 are close, but they describe a composer/author of written work, not a lyrics writer\n",
    "                    - other properties also describe something another than lyric's writer\n",
    "                    RELEVANT PROPERTIES: P676\n",
    "\n",
    "\n",
    "                    EXAMPLE#5\n",
    "                    QUESTION: In which country did the great Russian chess player Alexander Alekhine end his life?\n",
    "                    CANDIDATE PROPERTIES:\n",
    "                    P20: place of death -- most specific known (e.g. city instead of country, or hospital instead of city) death location of a person, animal or fictional character\n",
    "                    P27: country of citizenship -- the object is a country that recognizes the subject as its citizen\n",
    "                    P582: end time -- moment when an entity ceases to exist or a statement stops being valid\n",
    "                    P31: instance of -- that class of which this subject is a particular example and member; different from P279 (subclass of); for example: K2 is an instance of mountain; volcano is a subclass of mountain (and an instance of volcanic landform)\n",
    "                    P276: location -- location of the object, structure or event. In the case of an administrative entity as containing item use P131. For statistical entities use P8138. In the case of a geographic entity use P706. Use P7153 for locations associated with the object\n",
    "                    P26: spouse -- the subject has the object as their spouse (husband, wife, partner, etc.). Use \"unmarried partner\" (P451) for non-married companions\n",
    "                    P17: country -- sovereign state that this item is in (not to be used for human beings)\n",
    "                    P39: position held -- subject currently or formerly holds the object position or public office\n",
    "                    P580: start time -- time an entity begins to exist or a statement starts being valid\n",
    "                    P19: place of birth -- most specific known birth location of a person, animal or fictional character\n",
    "                    **Reasoning:**\n",
    "                    - the question asks about location of somebody's death and it's country where it happened\n",
    "                    - in the candidates we can clearly see P20 and P17, which respresent **place of death** and **contry**\n",
    "                    - P276 and P582 are close, but too common for this QUESTION\n",
    "                    - other has no relation to QUESTION's PROPERTIES\n",
    "                    RELEVANT PROPERTIES: P20, P17\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_PROP = f\"\"\"You are an experienced WikiData user. \n",
    "                   You receive a QUESTION and a SET of extracted Wikidata CANDIDATE PROPERTIES, retrieved from an index based on similarity between the QUESTION and the PROPERTY. \n",
    "                   Each property has a wikidata ID, label and description in a form of Wikidata ID: label -- description. \n",
    "                   Your task is to select only the most semantically relevant properties based on their labels and descriptions.\n",
    "                   \n",
    "                   Instruction:\n",
    "                    -- Ignore properties that are loosely related, homonyms, or have indirect contextual links to the QUESTION.\n",
    "                    -- If no property perfectly matches the question, select the best available property based on contextual closeness.\n",
    "                    -- If multiple properties seem relevant, prefer those with a stronger direct connection to the query subject.\n",
    "                    -- Some questions may have multiple correct properties. Return all properties that are directly relevant. \n",
    "                    -- Always include reasoning in the beginning of the responce.\n",
    "                    -- Always place \"RELEVANT PROPERTIES: ...\" at the end of your response.\n",
    "\n",
    "                   Here are examples:\n",
    "                   {FEW_SHOT_EXAMPLES_PROP}\n",
    "                   \"\"\"\n",
    "\n",
    "USER_PROMPT_PROP = \"\"\"QUESTION: {question}\\nCANDIDATE PROPERTIES:\\n{candidate_list_str}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0c7d7-dada-4812-864e-b75722c7f9bc",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fd709-f5a3-4761-8105-0a65da37d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prompt and Preprocessing Functions\n",
    "'''\n",
    "\n",
    "def get_default(dic, field, default=''):\n",
    "    return dic[field] if field in dic else default\n",
    "\n",
    "    \n",
    "def prepare_candidates(sample, retriever_output):    \n",
    "    sample_id = sample['uid']\n",
    "    # relevant_ids = list(sample['id2alias'].keys())\n",
    "    retrived_cands = retriever_output[str(sample_id)]\n",
    "    retrived_ids = retrived_cands.keys()\n",
    "    retrived_ids = list(filter(lambda x: x is not None, retrived_ids))\n",
    "    \n",
    "    candidates_set = list(set(retrived_ids\n",
    "                              # + relevant_ids\n",
    "                             ))\n",
    "    # HERE I DO SHUFFLING, SINCE I DO NOT KNOW THE ORDER FROM RETRIEVER\n",
    "    random.shuffle(candidates_set)\n",
    "    \n",
    "    entities_list = [f\"{key}: {get_default(retrived_cands[key], 'label')} -- {get_default(retrived_cands[key], 'description')}\" for key in retrived_ids]\n",
    "    entities_str = \"\\n\".join(entities_list)\n",
    "    return entities_str\n",
    "        \n",
    "\n",
    "def construct_user_prompt(sample, retriever_output, user_prompt):\n",
    "    question = sample['question_eng']\n",
    "    candidates_set_string = prepare_candidates(sample, retriever_output)\n",
    "    user_prompt = user_prompt.format(question=question, candidate_list_str=candidates_set_string)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc40b0-ec37-45c3-b0f6-78b3f0c5a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run OpenAI Functions\n",
    "'''\n",
    "def run_sample_with_openai(sample, retriever_output, system_prompt, user_prompt):\n",
    "    user_prompt = construct_user_prompt(sample, retriever_output, user_prompt)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,   # Reduce randomness for consistent entity selection\n",
    "        max_tokens=400,  # Adjust if the response is cut off\n",
    "        top_p=1,         # Avoid sampling randomness\n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0  \n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabae1f6-e71c-42c7-afc6-830f59fe3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Eval Functions\n",
    "'''\n",
    "def extract_wikidata_ids(text, search_pattern):\n",
    "    pattern = search_pattern + r\" ([\\w, ]+)\"\n",
    "    \n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the IDs and return them as a list\n",
    "        wikidata_ids = match.group(1).split(', ')\n",
    "        return wikidata_ids\n",
    "    else:\n",
    "        return []\n",
    "        \n",
    "# RELEVANT ENTITIES: / RELEVANT PROPERTIES:\n",
    "def parse_responce(responce_str, search_pattern):\n",
    "    if search_pattern == 'entity':\n",
    "        search_pattern = 'RELEVANT ENTITIES:'\n",
    "    elif search_pattern == 'property':\n",
    "        search_pattern = 'RELEVANT PROPERTIES:'\n",
    "    else:\n",
    "        raise Exception('Choose correct seacrh pattern')\n",
    "    \n",
    "    # parse main case\n",
    "    if search_pattern in responce_str:\n",
    "        wikidata_ids = extract_wikidata_ids(responce_str, search_pattern)\n",
    "        return wikidata_ids\n",
    "    # I hope there are no other cases, but still -- better to ch\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_metrics(eval_data, response_data, search_pattern):\n",
    "\n",
    "    if search_pattern == 'entity':\n",
    "        get_gold = lambda x: list(x['id2alias'].keys())\n",
    "    elif search_pattern == 'property':\n",
    "        get_gold = lambda x: [y.split(':')[-1] for y in x['question_props']]\n",
    "    else:\n",
    "        raise Exception('Choose correct seacrh pattern')\n",
    "    \n",
    "    overall_precision, overall_recall, overall_f1 = 0, 0, 0\n",
    "    failed_examples = []\n",
    "    error_generations, incomplete_generation = [], []\n",
    "    false_positive_generations = []\n",
    "    for idx, pair in enumerate(zip(eval_data, response_data)):\n",
    "        sample, gpt_response = pair\n",
    "        extracted_candidates = parse_responce(gpt_response, search_pattern)\n",
    "        \n",
    "        gold_ids = get_gold(sample)\n",
    "\n",
    "        \n",
    "        if extracted_candidates:\n",
    "            true_positives = set(extracted_candidates) & set(gold_ids)\n",
    "    \n",
    "            precision = len(true_positives) / len(extracted_candidates) if extracted_candidates else 0.0\n",
    "            if precision == 0:\n",
    "                false_positive_generations.append([idx, gpt_response, gold_ids])\n",
    "    \n",
    "            # Recall: Proportion of gold entities that are correctly predicted\n",
    "            recall = len(true_positives) / len(gold_ids) if gold_ids else 0.0\n",
    "            if recall == 0:\n",
    "                error_generations.append([idx, gpt_response, gold_ids])\n",
    "            if recall != 1:\n",
    "                incomplete_generation.append([idx, gpt_response, gold_ids])\n",
    "    \n",
    "            # F1-Score: Harmonic mean of Precision and Recall\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    \n",
    "            overall_precision += precision\n",
    "            overall_recall += recall\n",
    "            overall_f1 += f1\n",
    "        else:\n",
    "            failed_examples.append(gpt_response)\n",
    "        \n",
    "    overall_precision /= len(response_data)\n",
    "    overall_recall /= len(response_data)\n",
    "    overall_f1 /= len(response_data)\n",
    "    \n",
    "    print('Precision: ', overall_precision)\n",
    "    print('Recall: ', overall_recall)\n",
    "    print('F1: ', overall_f1)\n",
    "\n",
    "\n",
    "def get_result(data, retriver_output, response_arr, search_pattern):\n",
    "    result = {}\n",
    "    for idx, pair in enumerate(zip(data, response_arr)):\n",
    "        sample, gpt_response = pair\n",
    "        extracted_candidates = parse_responce(gpt_response, search_pattern)\n",
    "        uid = str(sample['uid'])\n",
    "        result[uid] = {\n",
    "            'question_eng': sample['question_eng'],\n",
    "            'query': sample['query'],\n",
    "            'candidates': {cand: retriver_output[uid][cand]['label']\n",
    "                           for cand in extracted_candidates if cand in retriver_output[uid]} if extracted_candidates else {}\n",
    "        }\n",
    "    return result\n",
    "\n",
    "    \n",
    "\n",
    "'''\n",
    "Test Functions\n",
    "'''\n",
    "def test_prompt(data, retriever_output, user_prompt, idx):\n",
    "    sample = data[idx]\n",
    "    print('SPARQL: ', sample['query'])\n",
    "    print()\n",
    "    result_user_prompt = construct_user_prompt(sample, retriever_output, user_prompt)\n",
    "    print(result_user_prompt[:])\n",
    "\n",
    "\n",
    "def test_openai(data, retriver_output, system_prompt, user_prompt, idx):\n",
    "    sample = data[idx]\n",
    "    print('QUESTION: ', sample['question_eng'])\n",
    "    print('SPARQL: ', sample['query'])\n",
    "    \n",
    "    openai_responce = run_sample_with_openai(sample, retriver_output, system_prompt, user_prompt)\n",
    "    print()\n",
    "    print(openai_responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769bf00-739d-4acc-9d38-8f4859c73b84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run Entitties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae502359-e391-494b-af71-588422247037",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_train = json.load(open(\"data/train_with_aliases.json\"))\n",
    "rubq_test = json.load(open(\"data/test_with_aliases.json\"))\n",
    "\n",
    "rubq_retr_100_ent = json.load(open('data/datasets/rubq_test_entities_retrieval.json'))\n",
    "rubq_retr_10_ent = {k: dict(list(v.items())[:10]) for k, v in rubq_retr_100_ent.items()}\n",
    "\n",
    "rubq_test_eval = [sample for sample in rubq_test if str(sample['uid']) in rubq_retr_100_ent]\n",
    "\n",
    "print(len(rubq_test), len(rubq_test_eval))\n",
    "\n",
    "print(\n",
    "    f'retr 100: {np.mean([len(set(x['id2alias'].keys()) & set(rubq_retr_100_ent[str(x['uid'])])) > 0 for x in rubq_test_eval])}',\n",
    "    f'retr 10 : {np.mean([len(set(x['id2alias'].keys()) & set(rubq_retr_10_ent[str(x['uid'])])) > 0 for x in rubq_test_eval])}',\n",
    "    sep='\\n'\n",
    ")\n",
    "\n",
    "eval_data = deepcopy(rubq_test_eval)\n",
    "eval_retriver = deepcopy(rubq_retr_10_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24270dc2-2355-4c18-a3e3-ba5439cae70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_prompt(eval_data, eval_retriver, USER_PROMPT_ENTS, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c9299-69e4-485f-a974-29edcdbfd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_openai(eval_data, eval_retriver, SYSTEM_PROMPT_ENTS, USER_PROMPT_ENTS, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46598869-20ff-4836-b8fe-86ea75ad48fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if responce_list:\n",
    "        start_from = i\n",
    "except NameError:\n",
    "    responce_list = []\n",
    "    start_from = 0\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(len(eval_data[:]))):\n",
    "\n",
    "        if i < start_from:\n",
    "            continue\n",
    "        \n",
    "        sample = eval_data[i]\n",
    "        result = run_sample_with_openai(sample, eval_retriver, SYSTEM_PROMPT_ENTS, USER_PROMPT_ENTS)\n",
    "        responce_list.append(result)\n",
    "except Exception as e:\n",
    "    print(i)\n",
    "    print('Перезапустите VPN и запустите ячейку')\n",
    "    raise e\n",
    "\n",
    "assert len(responce_list) == len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65156f80-d0fc-4129-8141-ba3ecde4ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responce_list = json.load(open('data/datasets/rubq_output_predicates_10.json'))\n",
    "# json.dump(responce_list, open('data/datasets/rubq_output_predicates_10.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a5f1c-3f15-4dab-9cc1-b23ca4fcab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(eval_data, responce_list, search_pattern='entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7c7c0-6c68-4b4a-b06b-8dcf93c32ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_result(eval_data, eval_retriver, responce_list, search_pattern='entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a0da6-a316-4408-9627-4b2c08a9324e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# json.dump(result, open('data/datasets/rubq_result_entities_100.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011df2c8-cb49-44e7-8e69-23a3dbb3bf0b",
   "metadata": {},
   "source": [
    "## Run Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e501eb-01a0-48fa-9ae5-03cf6bc7202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_train = json.load(open(\"data/train_with_aliases.json\"))\n",
    "rubq_test = json.load(open(\"data/test_with_aliases.json\"))\n",
    "\n",
    "rubq_retr_prop_100 = json.load(open('data/retriver_out/rubq/rubq_test_predicates_retrieval.json'))\n",
    "rubq_retr_prop_10 = {k: dict(list(v.items())[:10]) for k, v in rubq_retr_prop_100.items()}\n",
    "\n",
    "rubq_test_eval = [sample for sample in rubq_test if str(sample['uid']) in rubq_retr_prop_100]\n",
    "\n",
    "print(len(rubq_test), len(rubq_test_eval))\n",
    "\n",
    "print(\n",
    "    f'retr 100: {np.mean([len(set(\n",
    "        [y.split(':')[-1] for y in x['question_props']]\n",
    "    ) & set(rubq_retr_prop_100[str(x['uid'])])) > 0 for x in rubq_test_eval])}',\n",
    "    f'retr 10: {np.mean([len(set(\n",
    "        [y.split(':')[-1] for y in x['question_props']]\n",
    "    ) & set(rubq_retr_prop_10[str(x['uid'])])) > 0 for x in rubq_test_eval])}',\n",
    "    sep='\\n'\n",
    ")\n",
    "\n",
    "eval_data = deepcopy(rubq_test_eval)\n",
    "eval_retriver = deepcopy(rubq_retr_prop_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1b865-169c-4df5-bb26-a299c9f30966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_prompt(eval_data, eval_retriver, USER_PROMPT_PROP, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c27a2-11ea-4d0e-aa48-39d98776d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_openai(eval_data, eval_retriver, SYSTEM_PROMPT_PROP, USER_PROMPT_PROP, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95fbe4-8a28-4492-80f7-414b67eb6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if responce_list:\n",
    "        start_from = i\n",
    "except NameError:\n",
    "    responce_list = []\n",
    "    start_from = 0\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(len(eval_data[:]))):\n",
    "\n",
    "        if i < start_from:\n",
    "            continue\n",
    "        \n",
    "        sample = eval_data[i]\n",
    "        result = run_sample_with_openai(sample, eval_retriver, SYSTEM_PROMPT_PROP, USER_PROMPT_PROP)\n",
    "        responce_list.append(result)\n",
    "except Exception as e:\n",
    "    print(i)\n",
    "    print('Перезапустите VPN и запустите ячейку')\n",
    "    raise e\n",
    "\n",
    "assert len(responce_list) == len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955725bf-0605-4dd2-a1eb-0c46f254900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "responce_list = json.load(open('data/datasets/rubq_output_properties_10.json'))\n",
    "# json.dump(responce_list, open('data/datasets/rubq_output_properties_100.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a206806-0340-4db8-b1c8-3e9869562e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_metrics(eval_data, responce_list, search_pattern='property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c732ac8-b49a-4dd8-bff5-ebb63a0b168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_result(eval_data, eval_retriver, responce_list, search_pattern='property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30b53d-772b-4c68-8d69-192370a68336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json.dump(result, open('data/datasets/rubq_result_properties_10.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_chatgptapi",
   "language": "python",
   "name": "kernel_chatgptapi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
