{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare jsonl files with combined data (label + description + aliases + ...) for entities and predicates to build bm25 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKIDATA_FILES_PATH = 'data/wikidata_dump/processed_dump'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filter entities with less than 10 relations and/or no descriptions/aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ids = set()\n",
    "descriptions_ids = set()\n",
    "aliases_ids = set()\n",
    "\n",
    "path = f'{WIKIDATA_FILES_PATH}/labels'\n",
    "for filename in tqdm(os.listdir(path)):\n",
    "    with open(f'{path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            qid = data['qid']\n",
    "            labels_ids.add(qid)\n",
    "\n",
    "path = f'{WIKIDATA_FILES_PATH}/descriptions'\n",
    "for filename in tqdm(os.listdir(f'{WIKIDATA_FILES_PATH}/descriptions')):\n",
    "    with open(f'{path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            qid = data['qid']\n",
    "            descriptions_ids.add(qid)\n",
    "\n",
    "path = f'{WIKIDATA_FILES_PATH}/aliases'\n",
    "for filename in tqdm(os.listdir(path)):\n",
    "    with open(f'{path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            qid = data['qid']\n",
    "            aliases_ids.add(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entities without descriptions and aliases\n",
    "\n",
    "description_nans = labels_ids.difference(descriptions_ids)\n",
    "aliases_nans = labels_ids.difference(aliases_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(description_nans), len(aliases_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count relations for all entities\n",
    "\n",
    "relations_counts = {}\n",
    "path = f'{WIKIDATA_FILES_PATH}/entity_rels'\n",
    "\n",
    "for filename in tqdm(os.listdir(path)):\n",
    "    with open(f'{path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            qid = data['qid']\n",
    "            relations_counts[qid] = relations_counts.get(qid, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(relations_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will keep ids with 10 or more relations\n",
    "\n",
    "popular = [x for x, y in relations_counts.items() if y >= 10]\n",
    "popular_ids = labels_ids.intersection(set(popular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also keep all entities from datasets\n",
    "\n",
    "datasets_files = ['data/preprocessed/lcquad_2.0/lcquad_2.0_test.json',\n",
    "                  'data/preprocessed/lcquad_2.0/lcquad_2.0_train.json',\n",
    "                  'data/preprocessed/pat/pat_test.json',\n",
    "                  'data/preprocessed/pat/pat_train.json',\n",
    "                  'data/preprocessed/qald/qald_test.json',\n",
    "                  'data/preprocessed/qald/qald_train.json',\n",
    "                  'data/preprocessed/rubq/rubq_test.json',\n",
    "                  'data/preprocessed/rubq/rubq_train.json']\n",
    "\n",
    "dataset_ids = set()\n",
    "\n",
    "for file in tqdm(datasets_files):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        all_ids = data['entities']\n",
    "        for i in all_ids:\n",
    "            dataset_ids.add(i)\n",
    "\n",
    "len(dataset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only popular ids with descriptions or ids from datasets\n",
    "\n",
    "ids_to_keep = labels_ids.intersection(descriptions_ids)\n",
    "ids_to_keep = ids_to_keep.intersection(popular_ids)\n",
    "ids_to_keep = ids_to_keep.union(dataset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_nans = None\n",
    "\n",
    "with open('description_nan_ids.txt', 'r', encoding='utf-8') as f:\n",
    "    description_nans = set(f.read().splitlines())\n",
    "\n",
    "len(description_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_iterator(folder_path):\n",
    "    \"\"\"Iterate through all files from the folder in the order of numbers\"\"\"\n",
    "    files = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))\n",
    "    for file in files:\n",
    "        with open(f'{folder_path}/{file}', 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all files and find descriptions and labels for all entities from ids_to_keep\n",
    "# Save all to jsonl files to build bm25 index using pyserini\n",
    "# Here, we take advantage of the fact that all IDs in the files appear in the same order. The same IDs always appear in a row, and they only occur once for all files.\n",
    "# This code can be modified to add any other information about entities to combined data\n",
    "\n",
    "iterator_aliases = create_folder_iterator(f'{WIKIDATA_FILES_PATH}/aliases')\n",
    "iterator_descriptions = create_folder_iterator(f'{WIKIDATA_FILES_PATH}/descriptions')\n",
    "\n",
    "labels_files = sorted(os.listdir(f'{WIKIDATA_FILES_PATH}/labels'), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "current_aliases_data = None\n",
    "current_descriptions_data = None\n",
    "current_entity_data = None\n",
    "\n",
    "save_folder_path = 'data/combined_data'\n",
    "path = f'{WIKIDATA_FILES_PATH}/labels'\n",
    "\n",
    "os.mkdir(save_folder_path)\n",
    "\n",
    "file_num = 0\n",
    "ids_data = []\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for file in tqdm(labels_files):\n",
    "    with open(f'{path}/{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            qid, label = data['qid'], data['label']\n",
    "\n",
    "            if qid not in ids_to_keep:\n",
    "                continue\n",
    "            \n",
    "            aliases = []\n",
    "            description = None\n",
    "            entities = set()\n",
    "            \n",
    "            # descriptions\n",
    "            if qid not in description_nans:\n",
    "                while True:\n",
    "                    try:\n",
    "                        current_descriptions_data = next(iterator_descriptions)\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    \n",
    "                    if current_descriptions_data['qid'] != qid:\n",
    "                        continue\n",
    "                    elif current_descriptions_data['qid'] == qid:\n",
    "                        description = current_descriptions_data['description']\n",
    "                        break\n",
    "            else:\n",
    "                description = ''\n",
    "\n",
    "            if description is None:\n",
    "                raise RuntimeError(f'Description for id {qid} not found')\n",
    "            \n",
    "            aliases_found = False\n",
    "\n",
    "            # aliases\n",
    "            while True:\n",
    "                if current_aliases_data is None:\n",
    "                    try:\n",
    "                        current_aliases_data = next(iterator_aliases)\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                \n",
    "                if current_aliases_data['qid'] != qid:\n",
    "                    if aliases_found:\n",
    "                        break\n",
    "                    else:\n",
    "                        try:\n",
    "                            current_aliases_data = next(iterator_aliases)\n",
    "                        except StopIteration:\n",
    "                            break\n",
    "                        continue\n",
    "                elif current_aliases_data['qid'] == qid:\n",
    "                    aliases_found = True\n",
    "                    aliases.append(current_aliases_data['alias'])\n",
    "                    try:\n",
    "                        current_aliases_data = next(iterator_aliases)\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            if aliases == []:\n",
    "                raise RuntimeError(f'Aliases for id {qid} not found')\n",
    "            \n",
    "            id_result = f'{label} {description}'\n",
    "\n",
    "            for a in aliases:\n",
    "                id_result += f' {a}'\n",
    "\n",
    "            for e in entities:\n",
    "                id_result += f' {relations_descriptions[e]}'\n",
    "\n",
    "            ids_data.append({'id': qid, 'contents': id_result})\n",
    "\n",
    "            if len(ids_data) == 10000:\n",
    "                output_path = f'{save_folder_path}/{file_num}.jsonl'\n",
    "                file_num += 1\n",
    "                with open(output_path, 'w') as jsonl_file:\n",
    "                    for entry in ids_data:\n",
    "                        jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "                ids_data = []\n",
    "\n",
    "if len(ids_data) > 0:\n",
    "    output_path = f'{save_folder_path}/{file_num}.jsonl'\n",
    "    file_num += 1\n",
    "    with open(output_path, 'w') as jsonl_file:\n",
    "        for entry in ids_data:\n",
    "            jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "    ids_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_data = {}\n",
    "path = f'{WIKIDATA_FILES_PATH}/descriptions'\n",
    "for filename in tqdm(os.listdir(f'{WIKIDATA_FILES_PATH}/descriptions')):\n",
    "    with open(f'{path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            qid = data['qid']\n",
    "            if qid not in ids_to_keep:\n",
    "                continue\n",
    "            descriptions_data[qid] = data['description'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikidata_relations_info.json', 'r', encoding='utf-8') as f:\n",
    "    relations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add information about relations and related entities descriptions\n",
    "\n",
    "data_files = sorted(os.listdir('data/combined_data'), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "file_num = 0\n",
    "ids_data = []\n",
    "\n",
    "save_folder_path = 'data/combined_data_with_rels'\n",
    "path = 'data/combined_data'\n",
    "\n",
    "os.mkdir(save_folder_path)\n",
    "\n",
    "iterator_entity_rels = create_folder_iterator(f'{WIKIDATA_FILES_PATH}/entity_rels')\n",
    "\n",
    "current_rels_data = None\n",
    "current_qid, current_pid, current_value = None, None, None\n",
    "\n",
    "file_num = 0\n",
    "ids_data = []\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for file in tqdm(data_files):\n",
    "    with open(f'{path}/{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            qid, content = data['id'], data['contents']\n",
    "            if qid in nan_ids:\n",
    "                ids_data.append({'id': qid, 'contents': content})\n",
    "                continue\n",
    "\n",
    "            data_found = False\n",
    "            predicates = []\n",
    "            entities = []\n",
    "            while True:\n",
    "                if current_rels_data is None:\n",
    "                    try:\n",
    "                        current_rels_data = next(iterator_entity_rels)\n",
    "                        current_qid = current_rels_data['qid']\n",
    "                        current_pid = current_rels_data['pid']\n",
    "                        current_value = current_rels_data['value']\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                \n",
    "                if current_qid != qid:\n",
    "                    if data_found:\n",
    "                        break\n",
    "                    else:\n",
    "                        try:\n",
    "                            current_rels_data = next(iterator_entity_rels)\n",
    "                            current_qid = current_rels_data['qid']\n",
    "                            current_pid = current_rels_data['pid']\n",
    "                            current_value = current_rels_data['value']\n",
    "                        except StopIteration:\n",
    "                            break\n",
    "                        continue\n",
    "                elif current_qid == qid:\n",
    "                    data_found = True\n",
    "                    predicates.append(current_pid)\n",
    "                    entities.append(current_value)\n",
    "                    try:\n",
    "                        current_rels_data = next(iterator_entity_rels)\n",
    "                        current_qid = current_rels_data['qid']\n",
    "                        current_pid = current_rels_data['pid']\n",
    "                        current_value = current_rels_data['value']\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            if predicates == [] and entities == []:\n",
    "                raise ValueError(f'Error {qid}')\n",
    "\n",
    "            p_text = []\n",
    "\n",
    "            for p in predicates:\n",
    "                if p in relations_data:\n",
    "                    p_label = relations_data[p]['label']\n",
    "                    p_desc = relations_data[p]['description']\n",
    "                    if not (p_label is None):\n",
    "                        p_text.append(p_label)\n",
    "                    if not (p_desc is None):\n",
    "                        p_text.append(p_desc)\n",
    "            \n",
    "            if p_text:\n",
    "                content = f'{content} {' '.join(p_text)}'\n",
    "\n",
    "            q_text = []\n",
    "            for q in entities:\n",
    "                 if q in descriptions_data:\n",
    "                    q_text.append(descriptions_data[q])\n",
    "            \n",
    "            if q_text:\n",
    "                content = f'{content} {' '.join(q_text)}'\n",
    "\n",
    "            ids_data.append({'id': qid, 'contents': content})\n",
    "\n",
    "        \n",
    "        output_path = f'{save_folder_path}/{file_num}.json'\n",
    "        file_num += 1\n",
    "        with open(output_path, 'w') as jsonl_file:\n",
    "            for entry in ids_data:\n",
    "                jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "        ids_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build combined data for Predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each predicate, we save its label, description, aliases, and questions from the datasets in which it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed/rubq/rubq_train.json', 'r', encoding='utf-8') as f:\n",
    "    rubq_data = json.load(f)\n",
    "\n",
    "with open('data/preprocessed/qald/qald_train.json', 'r', encoding='utf-8') as f:\n",
    "    qald_data = json.load(f)\n",
    "\n",
    "with open('data/preprocessed/pat/pat_train.json', 'r', encoding='utf-8') as f:\n",
    "    pat_data = json.load(f)\n",
    "\n",
    "with open('data/preprocessed/lcquad_2.0/lcquad_2.0_train.json', 'r', encoding='utf-8') as f:\n",
    "    lcquad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_data = rubq_data['dataset']\n",
    "qald_data = qald_data['dataset']\n",
    "pat_data = pat_data['dataset']\n",
    "lcquad_data = lcquad_data['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_questions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in rubq_data:\n",
    "    relations = []\n",
    "    if entity['relations']['question']:\n",
    "        for r in entity['relations']['question']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    if entity['relations']['query']:\n",
    "        for r in entity['relations']['query']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    for r in relations:\n",
    "        if r not in predicates_questions:\n",
    "            predicates_questions[r] = set()\n",
    "        predicates_questions[r].add(entity['en_question'])\n",
    "\n",
    "for entity in qald_data:\n",
    "    relations = []\n",
    "    if entity['relations']['question']:\n",
    "        for r in entity['relations']['question']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    if entity['relations']['query']:\n",
    "        for r in entity['relations']['query']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    for r in relations:\n",
    "        if r not in predicates_questions:\n",
    "            predicates_questions[r] = set()\n",
    "        predicates_questions[r].add(entity['en_question'])\n",
    "\n",
    "for entity in pat_data:\n",
    "    relations = []\n",
    "    if entity['relations']['question']:\n",
    "        for r in entity['relations']['question']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    if entity['relations']['query']:\n",
    "        for r in entity['relations']['query']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    for r in relations:\n",
    "        if r not in predicates_questions:\n",
    "            predicates_questions[r] = set()\n",
    "        predicates_questions[r].add(entity['en_question'])\n",
    "\n",
    "for entity in lcquad_data:\n",
    "    relations = []\n",
    "    if entity['relations']['question']:\n",
    "        for r in entity['relations']['question']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    if entity['relations']['query']:\n",
    "        for r in entity['relations']['query']:\n",
    "            if r not in relations:\n",
    "                relations.append(r)\n",
    "    for r in relations:\n",
    "        if r not in predicates_questions:\n",
    "            predicates_questions[r] = set()\n",
    "        predicates_questions[r].add(entity['en_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikidata_relations_info.json', 'r', encoding='utf-8') as f:\n",
    "    predicates_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_full = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for p in predicates_data:\n",
    "    info = f'{predicates_data[p]['label']} {predicates_data[p]['description']} {' '.join(predicates_data[p]['aliases'])}'\n",
    "    if p in predicates_questions:\n",
    "        info += f' {' '.join(predicates_questions[p])}'\n",
    "    else:\n",
    "        cnt += 1\n",
    "    predicates_full[p] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_jsonl = []\n",
    "\n",
    "for key in tqdm(predicates_full):\n",
    "    predicates_jsonl.append({'id': key, 'contents': predicates_full[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/combined_predicates_data/0.jsonl', 'w') as jsonl_file:\n",
    "    for entry in predicates_jsonl:\n",
    "        jsonl_file.write(json.dumps(entry) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-kgqa-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
