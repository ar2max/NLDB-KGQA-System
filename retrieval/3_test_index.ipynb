{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BM25 Index on all datasets and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def test_bm25(index_dir, test_data_path, k1=0.4, b=0.4, topk=10, predict_type='entities', tqdm_desc=\"batches\", verbose=True):\n",
    "    '''Search for candidates using bm25 index and count metrics'''\n",
    "\n",
    "    searcher = LuceneSearcher(index_dir)\n",
    "    searcher.set_bm25(k1, b)\n",
    "\n",
    "    rubq_test = json.load(\n",
    "        open(test_data_path))['dataset']\n",
    "\n",
    "\n",
    "    overall_precision, overall_recall, overall_f1 = 0, 0, 0\n",
    "    samples_with_sparql = 0\n",
    "    predicted_candidates = dict()\n",
    "\n",
    "    for sample in tqdm(rubq_test, total=len(rubq_test), desc=tqdm_desc, display=verbose):\n",
    "        query = sample[\"en_question\"]\n",
    "        query_id = sample['id']\n",
    "        if not (predict_type in sample and sample[predict_type]['query']):\n",
    "            continue\n",
    "        gold_ids = list(sample[predict_type]['query'].keys())\n",
    "\n",
    "        samples_with_sparql += 1\n",
    "\n",
    "        result = searcher.search(query, k=topk)\n",
    "\n",
    "        predicted_ids = []\n",
    "        for res in result:\n",
    "            doc_id = res.docid\n",
    "            predicted_ids.append(doc_id)\n",
    "\n",
    "        predicted_candidates[query_id] = predicted_ids\n",
    "\n",
    "        true_positives = set(predicted_ids) & set(gold_ids)\n",
    "\n",
    "        precision = len(true_positives) / len(predicted_ids) if predicted_ids else 0.0\n",
    "\n",
    "        # Recall: Proportion of gold entities that are correctly predicted\n",
    "        recall = len(true_positives) / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "        # F1-Score: Harmonic mean of Precision and Recall\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "        overall_precision += precision\n",
    "        overall_recall += recall\n",
    "        overall_f1 += f1\n",
    "\n",
    "    overall_precision /= samples_with_sparql\n",
    "    overall_recall /= samples_with_sparql\n",
    "    overall_f1 /= samples_with_sparql\n",
    "\n",
    "    return predicted_candidates, overall_precision, overall_recall, overall_f1\n",
    "\n",
    "\n",
    "def save_predicates_retriever_result(path, predicted_candidates):\n",
    "    '''Save predicates candidates with their label and description'''\n",
    "\n",
    "    with open('data/wikidata_relations_info.json', 'r', encoding='utf-8') as f:\n",
    "        predicates_data = json.load(f)\n",
    "    \n",
    "    predicates_result = {}\n",
    "    for index in predicted_candidates:\n",
    "        predicates_result[index] = {}\n",
    "        for p in predicted_candidates[index]:\n",
    "            predicates_result[index][p] = {'label': predicates_data[p]['label'], 'description': predicates_data[p]['description']}\n",
    "\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(predicates_result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print('OK')\n",
    "\n",
    "\n",
    "def save_entities_retriever_result(path, predicted_candidates, candidates_labels):\n",
    "    '''Save entities candidates with their label and description'''\n",
    "    predicted_candidates_result = {}\n",
    "\n",
    "    for key in predicted_candidates:\n",
    "        predicted_candidates_result[key] = {}\n",
    "        for qid in predicted_candidates[key]:\n",
    "            predicted_candidates_result[key][qid] = candidates_labels[qid]\n",
    "\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(predicted_candidates_result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print('OK')\n",
    "\n",
    "\n",
    "def bm25_params_grid_search(index_dir, test_data_path, topk=10, predict_type='entities'):\n",
    "    '''Search for optimal bm25 hyperparameters'''\n",
    "\n",
    "    #k1_grid = np.logspace(-1, 1, 50)\n",
    "    k1_grid = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, \n",
    "               1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, \n",
    "               6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0]\n",
    "    b_grid = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    grid = list(itertools.product(k1_grid, b_grid))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    best_recall = 0\n",
    "    best_k1 = 0\n",
    "    best_b = 0\n",
    "\n",
    "    for k1, b in tqdm(grid):\n",
    "        _, overall_precision, overall_recall, overall_f1 = test_bm25(index_dir, test_data_path, k1=k1, b=b, topk=topk, predict_type=predict_type, verbose=False)\n",
    "        results[str((k1, b))] = {'precision': overall_precision, 'recall': overall_recall, 'f1': overall_f1}\n",
    "        if overall_recall > best_recall:\n",
    "            best_recall = overall_recall\n",
    "            best_k1 = k1\n",
    "            best_b = b\n",
    "            print(f'best_recall: {best_recall}, k1: {k1}, b: {b}')\n",
    "\n",
    "    return best_k1, best_b\n",
    "\n",
    "\n",
    "def get_candidates_labels():\n",
    "    '''Get labels and descriptions for candidates from wikidata files'''\n",
    "\n",
    "    candidates_labels = {}\n",
    "\n",
    "    ids_to_keep = None\n",
    "\n",
    "    with open('ids_to_keep.txt', 'r', encoding='utf-8') as f:\n",
    "        ids_to_keep = set(f.read().splitlines())\n",
    "\n",
    "    labels_path='data/wikidata_dump/processed_dump/labels'\n",
    "\n",
    "    for filename in tqdm(os.listdir(labels_path), desc='reading labels'):\n",
    "        with open(f'{labels_path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                qid = data['qid']\n",
    "                if qid in ids_to_keep:\n",
    "                    candidates_labels[qid] = {'label': data['label']}\n",
    "\n",
    "    descriptions_path = 'data/wikidata_dump/processed_dump/descriptions'\n",
    "\n",
    "    for filename in tqdm(os.listdir(descriptions_path), desc='reading descriptions'):\n",
    "        with open(f'{descriptions_path}/{filename}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                qid = data['qid']\n",
    "                if qid in candidates_labels:\n",
    "                    candidates_labels[qid]['description'] = data['description']\n",
    "\n",
    "    return candidates_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_INDEX_DIR = \"combined_data_index\"\n",
    "PREDICATES_INDEX_DIR = \"predicates_index\"\n",
    "\n",
    "LCQUAD_PATH = 'data/preprocessed/lcquad_2.0/lcquad_2.0_test.json'\n",
    "QALD_PATH = 'data/preprocessed/qald/qald_test.json'\n",
    "PAT_PATH = 'data/preprocessed/pat/pat_test.json'\n",
    "RUBQ_PATH = 'data/preprocessed/rubq/rubq_test.json'\n",
    "\n",
    "RETRIEVAL_RESULT_SAVE_DIR = 'retrieval/retriever_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels and descriptions for candidates from wikidata files \n",
    "\n",
    "candidates_labels = get_candidates_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCQUAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for optimal hyperparameters\n",
    "\n",
    "best_k1, best_b = bm25_params_grid_search(ENTITIES_INDEX_DIR, LCQUAD_PATH, topk=10, predict_type='entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions and metrics\n",
    "\n",
    "lcquad_entities_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(ENTITIES_INDEX_DIR, LCQUAD_PATH, k1=2.947, b=0.2, predict_type='entities', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "# save results\n",
    "\n",
    "save_entities_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/lcquad_test_entities_retrieval.json', lcquad_entities_predicted_candidates, candidates_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(RELATIONS_INDEX_DIR, LCQUAD_PATH, topk=10, predict_type='relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcquad_relations_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(RELATIONS_INDEX_DIR, LCQUAD_PATH, k1=5.18, b=0.01, predict_type='relations', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "save_predicates_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/lcquad_test_predicates_retrieval.json', lcquad_relations_predicted_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QALD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(ENTITIES_INDEX_DIR, QALD_PATH, topk=10, predict_type='entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qald_entities_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(ENTITIES_INDEX_DIR, QALD_PATH, k1=2.95, b=0.2, predict_type='entities', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(RELATIONS_INDEX_DIR, QALD_PATH, topk=10, predict_type='relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qald_relations_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(RELATIONS_INDEX_DIR, QALD_PATH, k1=5.18, b=0.01, predict_type='relations', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "save_predicates_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/qald_test_predicates_retrieval.json', qald_relations_predicted_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(ENTITIES_INDEX_DIR, PAT_PATH, topk=10, predict_type='entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_entities_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(ENTITIES_INDEX_DIR, PAT_PATH, k1=1.0, b=0.7, predict_type='entities', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "save_entities_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/pat_test_entities_retrieval.json', pat_entities_predicted_candidates, candidates_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(PREDICATES_INDEX_DIR, PAT_PATH, topk=10, predict_type='relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_relations_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(PREDICATES_INDEX_DIR, PAT_PATH, k1=0.1, b=0.01, predict_type='relations', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "save_predicates_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/pat_test_predicates_retrieval.json', pat_relations_predicted_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUBQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(ENTITIES_INDEX_DIR, RUBQ_PATH, topk=10, predict_type='entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_entities_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(ENTITIES_INDEX_DIR, RUBQ_PATH, k1=1.39, b=0.4, predict_type='entities', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "save_entities_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/rubq_test_entities_retrieval.json', rubq_entities_predicted_candidates, candidates_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k1, best_b = bm25_params_grid_search(RELATIONS_INDEX_DIR, RUBQ_PATH, topk=10, predict_type='relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_relations_predicted_candidates, overall_precision, overall_recall, overall_f1 = test_bm25(RELATIONS_INDEX_DIR, RUBQ_PATH, k1=5.18, b=0.01, predict_type='relations', topk=100)\n",
    "\n",
    "print('Precision: ', overall_precision)\n",
    "print('Recall: ', overall_recall)\n",
    "print('F1: ', overall_f1)\n",
    "\n",
    "save_predicates_retriever_result(f'{RETRIEVAL_RESULT_SAVE_DIR}/rubq_test_predicates_retrieval.json', rubq_relations_predicted_candidates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-kgqa-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
